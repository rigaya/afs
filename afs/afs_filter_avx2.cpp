#define USE_AVX   1
#define USE_AVX2  1
#include <emmintrin.h>
#include <smmintrin.h>
#include <immintrin.h> //AVX2
#define WIN32_LEAN_AND_MEAN
#define NOMINMAX
#include <Windows.h>
#include "filter.h"
#include "simd_util.h"
#include "afs.h"

#if _MSC_VER >= 1800 && !defined(__AVX__) && !defined(_DEBUG)
static_assert(false, "do not forget to set /arch:AVX or /arch:AVX2 for this file.");
#endif

static const _declspec(align(32)) BYTE pqb_mask_a[] = {
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00,
    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
};

static const _declspec(align(32)) BYTE pqb_mask_s[] = {
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x00,
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff,
};
static const _declspec(align(32)) BYTE pb_mask_03[] = {
    0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 
    0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 
};
static const _declspec(align(32)) BYTE pb_mask_04[] = {
    0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 
    0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04, 
};

static const _declspec(align(32)) BYTE pb_mask_33[] = {
    0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 
    0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 0x33, 
};
static const _declspec(align(32)) BYTE pb_mask_f3[] = {
    0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 
    0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 0xf3, 
};
static const _declspec(align(32)) BYTE pb_mask_44[] = {
    0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 
    0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 0x44, 
};

void __stdcall afs_analyzemap_filter_avx2(BYTE* sip, int si_w, int w, int h) {
    __m256i y0, y1, y2, y3, y4, y5;
    __m256i y6 = _mm256_load_si256((__m256i*)(pb_mask_03));
    __m256i y7 = _mm256_load_si256((__m256i*)(pb_mask_04));
    BYTE *ptr_sip_out = sip;
    BYTE *ptr_sip_in = 0;

    //loop horizontal_1
    for (int jh = h; jh; jh--) {
        int iw = w;
        y1 = _mm256_loadu_si256((__m256i*)(ptr_sip_out));
        y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_out+32));
        iw -= 32;

        y3 = _mm256_alignr256_epi8(y1, y1, 31);
        y3 = _mm256_blendv_epi8(y3, y1, _mm256_load_si256((__m256i*)(pqb_mask_s + 1*32)));
        y4 = _mm256_alignr256_epi8(y2, y1, 1);
        
        y5 = y3;
        y3 = _mm256_or_si256(y3, y4);
        y5 = _mm256_and_si256(y5, y4);
        y3 = _mm256_and_si256(y3, y6);
        y5 = _mm256_and_si256(y5, y7);
        y3 = _mm256_or_si256(y3, y1);
        y3 = _mm256_or_si256(y3, y5);
        
        _mm256_storeu_si256((__m256i*)ptr_sip_out, y3);
        ptr_sip_in = ptr_sip_out + 32;
        
        for (; iw > 32; iw -= 32) {
            y0 = y1;
            y1 = y2;
            y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_in+1*32));
            
            y3 = _mm256_alignr256_epi8(y1, y0, 31);
            y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
            y5 = y3;
            y3 = _mm256_or_si256(y3, y4);
            y5 = _mm256_and_si256(y5, y4);
            y3 = _mm256_and_si256(y3, y6);
            y5 = _mm256_and_si256(y5, y7);
            y3 = _mm256_or_si256(y3, y1);
            y3 = _mm256_or_si256(y3, y5);
            _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
            
            ptr_sip_in += 32;
        }
        
        y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
        y4 = y2;
        y4 = _mm256_and_si256(y4, y0);
        y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
        y3 = _mm256_alignr256_epi8(y2, y1, 31);
        y5 = y2;
        y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
        y4 = _mm256_or_si256(y4, y5);
        
        y5 = y3;
        y3 = _mm256_or_si256(y3, y4);
        y5 = _mm256_and_si256(y5, y4);
        y3 = _mm256_and_si256(y3, y6);
        y5 = _mm256_and_si256(y5, y7);
        y3 = _mm256_or_si256(y3, y2);
        y3 = _mm256_or_si256(y3, y5);
        y3 = _mm256_and_si256(y3, y0);
        
        _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
        ptr_sip_out += si_w;
    }
    
    //loop vertical_1
    ptr_sip_out = sip;
    y7 = _mm256_or_si256(y7, y6);
    for (int jw = si_w >> 5; jw; jw--) {
        int ih = h;
        y1 = _mm256_loadu_si256((__m256i*)(ptr_sip_out));
        y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_out+si_w));
        ih -= 2;
        y3 = y2;
        y3 = _mm256_and_si256(y3, y1);
        y3 = _mm256_and_si256(y3, y7);
        y3 = _mm256_or_si256(y3, y1);
        _mm256_storeu_si256((__m256i*)ptr_sip_out, y3);
        ptr_sip_in = ptr_sip_out + si_w;
        for ( ; ih; ih--) {
            y0 = y1;
            y1 = y2;
            y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_in+si_w));
            
            y3 = y2;
            y3 = _mm256_and_si256(y3, y0);
            y3 = _mm256_and_si256(y3, y7);
            y3 = _mm256_or_si256(y3, y1);
            _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
            ptr_sip_in += si_w;
        }
        y3 = y2;
        y3 = _mm256_and_si256(y3, y1);
        y3 = _mm256_and_si256(y3, y7);
        y3 = _mm256_or_si256(y3, y2);
        _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
        ptr_sip_out += 32;
    }
    
    //loop horizontal_2
    ptr_sip_out = sip;
    y6 = _mm256_cmpeq_epi8(y6, y6);
    y6 = _mm256_xor_si256(y6, y7);
    for (int jh = h; jh; jh--) {
        int iw = w;
        y1 = _mm256_loadu_si256((__m256i*)(ptr_sip_out));
        y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_out+32));
        iw -= 32;

        y3 = _mm256_alignr256_epi8(y1, y1, 31);
        y3 = _mm256_blendv_epi8(y3, y1, _mm256_load_si256((__m256i*)(pqb_mask_s + 1*32)));
        y4 = _mm256_alignr256_epi8(y2, y1, 1);
        
        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y1);
        _mm256_storeu_si256((__m256i*)ptr_sip_out, y3);
        ptr_sip_in = ptr_sip_out + 32;
        
        for ( ; iw > 32; iw -= 32) {
            y0 = y1;
            y1 = y2;
            y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_in+32));
            
            y3 = _mm256_alignr256_epi8(y1, y0, 31);
            y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
            y3 = _mm256_and_si256(y3, y4);
            y3 = _mm256_or_si256(y3, y6);
            y3 = _mm256_and_si256(y3, y1);
            _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
            
            ptr_sip_in += 32;
        }
        
        y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
        
        y4 = y2;
        y4 = _mm256_and_si256(y4, y0);
        y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
        y3 = _mm256_alignr256_epi8(y2, y1, 31);
        y5 = y2;
        y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
        y4 = _mm256_or_si256(y4, y5);
        
        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y2);
        y3 = _mm256_and_si256(y3, y0);
        
        _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
        ptr_sip_out += si_w;
    }
    
    //loop vertical_2
    ptr_sip_out = sip;
    for (int jw = si_w >> 5; jw; jw--) {
        int ih = h;
        y1 = _mm256_loadu_si256((__m256i*)(ptr_sip_out));
        y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_out+si_w));
        ih -= 2;
        y3 = y2;
        y3 = _mm256_and_si256(y3, y1);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y1);
        _mm256_storeu_si256((__m256i*)ptr_sip_out, y3);
        ptr_sip_in = ptr_sip_out + si_w;
        for ( ; ih; ih--) {
            y0 = y1;
            y1 = y2;
            y2 = _mm256_loadu_si256((__m256i*)(ptr_sip_in+si_w));
            
            y3 = y2;
            y3 = _mm256_and_si256(y3, y0);
            y3 = _mm256_or_si256(y3, y6);
            y3 = _mm256_and_si256(y3, y1);
            _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
            ptr_sip_in += si_w;
        }
        y3 = y2;
        y3 = _mm256_and_si256(y3, y1);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y2);
        _mm256_storeu_si256((__m256i*)ptr_sip_in, y3);
        ptr_sip_out += 32;
    }
    _mm256_zeroupper();
}

 void __stdcall afs_merge_scan_avx2(BYTE* dst, BYTE* src0, BYTE* src1, int w, int si_w, int h, int x_start, int x_fin, int tb_order, int *stripe_count, AFS_SCAN_CLIP *mc_clip) {
    int step = x_start;
    __m256i y0, y1, y2, y3, y4, y5;
    const __m256i xPbMask33 = _mm256_load_si256((__m256i*)(pb_mask_33));
    const __m256i xPbMaskf3 = _mm256_load_si256((__m256i*)(pb_mask_f3));
    for (int jw = (x_fin - x_start) >> 5; jw; jw--) {
        BYTE *ptr_dst = dst  + step;
        BYTE *ptr_src0 = src0 + step;
        BYTE *ptr_src1 = src1 + step;
        y0 = _mm256_loadu_si256((__m256i*)(ptr_src0 + 0));
        y1 = _mm256_loadu_si256((__m256i*)(ptr_src0 + si_w));
        y2 = _mm256_loadu_si256((__m256i*)(ptr_src1 + 0));
        y3 = _mm256_loadu_si256((__m256i*)(ptr_src1 + si_w));
        ptr_src0 += si_w;
        ptr_src1 += si_w;
        _mm_prefetch((char *)(ptr_src0 + si_w), _MM_HINT_NTA);
        y4 = y0;
        y4 = _mm256_or_si256(y4, y1);
        y4 = _mm256_or_si256(y4, xPbMaskf3);
        y4 = _mm256_and_si256(y4, y0);
        y5 = y2;
        y5 = _mm256_or_si256(y5, y3);
        y5 = _mm256_or_si256(y5, xPbMaskf3);
        y5 = _mm256_and_si256(y5, y2);
        y4 = _mm256_and_si256(y4, y5);
        y4 = _mm256_and_si256(y4, _mm256_load_si256((__m256i*)pb_mask_44));
        _mm_prefetch((char *)(ptr_src1 + si_w), _MM_HINT_NTA);
        y5 = y0;
        y5 = _mm256_andnot_si256(y5, xPbMask33);
        y4 = _mm256_or_si256(y4, y5);
        _mm256_storeu_si256((__m256i*)ptr_dst, y4);
        ptr_dst += si_w;


        for (int ih = h - 2; ih; ih--) {
            y4 = y0;
            y0 = y1;
            y1 = _mm256_loadu_si256((__m256i*)(ptr_src0 + si_w));
            ptr_src0 += si_w;
            _mm_prefetch((char *)(ptr_src0 + si_w), _MM_HINT_NTA);
            y4 = _mm256_or_si256(y4, y1);
            y4 = _mm256_or_si256(y4, xPbMaskf3);
            y4 = _mm256_and_si256(y4, y0);
            y5 = y2;
            y2 = y3;
            
            y3 = _mm256_loadu_si256((__m256i*)(ptr_src1 + si_w));
            ptr_src1 += si_w;
            _mm_prefetch((char *)(ptr_src1 + si_w), _MM_HINT_NTA);
            y5 = _mm256_or_si256(y5, y3);
            y5 = _mm256_or_si256(y5, xPbMaskf3);
            y5 = _mm256_and_si256(y5, y2);
            y4 = _mm256_and_si256(y4, y5);
            y4 = _mm256_and_si256(y4, _mm256_load_si256((__m256i*)pb_mask_44));
            y5 = y0;
            y5 = _mm256_andnot_si256(y5, xPbMask33);
            y4 = _mm256_or_si256(y4, y5);
            _mm256_storeu_si256((__m256i*)ptr_dst, y4);
            ptr_dst += si_w;
        }
        
        y4 = y0;
        y4 = _mm256_or_si256(y4, y1);
        y4 = _mm256_or_si256(y4, xPbMaskf3);
        y4 = _mm256_and_si256(y4, y1);
        y5 = y2;
        y5 = _mm256_or_si256(y5, y3);
        y5 = _mm256_or_si256(y5, xPbMaskf3);
        y5 = _mm256_and_si256(y5, y3);
        y4 = _mm256_and_si256(y4, y5);
        y4 = _mm256_and_si256(y4, _mm256_load_si256((__m256i*)pb_mask_44));
        y5 = y1;
        
        y5 = _mm256_andnot_si256(y5, xPbMask33);
        y4 = _mm256_or_si256(y4, y5);
        _mm256_storeu_si256((__m256i*)ptr_dst, y4);
        
        step += 32;
    }
    _mm256_zeroupper();
 }
 
 void __stdcall afs_merge_scan_avx2_plus(BYTE* dst, BYTE* src0, BYTE* src1, int w, int si_w, int h, int x_start, int x_fin, int tb_order, int *stripe_count, AFS_SCAN_CLIP *mc_clip) {
    __m256i y0, y1, y2, y3, y4, y5;
    const __m256i xPbMask33 = _mm256_load_si256((__m256i*)(pb_mask_33));
    const __m256i xPbMaskf3 = _mm256_load_si256((__m256i*)(pb_mask_f3));
    const __m256i xPbMask44 = _mm256_load_si256((__m256i*)(pb_mask_44));
    const int MAX_BLOCK_SIZE = 1920;
    const int block_size = (si_w / (((si_w + MAX_BLOCK_SIZE-1) / MAX_BLOCK_SIZE)) + 31) & ~31;
    BYTE __declspec(align(32)) buffer[MAX_BLOCK_SIZE * 4];
    BYTE *bufptr_0 = buffer;
    BYTE *bufptr_1 = buffer + (block_size<<1);
    for (int jw = x_start, step; jw < x_fin; jw += block_size) {
        step = x_fin - jw;
        int over_block_size = 0-(step > block_size);
        step = (over_block_size & block_size) | ((~over_block_size) & step);
        BYTE *ptr_dst = dst  + jw;
        BYTE *ptr_src0 = src0 + jw;
        BYTE *ptr_src1 = src1 + jw;
        for (int iw = 0; iw < step; iw += 32) {
            y0 = _mm256_load_si256((__m256i*)(ptr_src0+iw+ 0));
            y1 = _mm256_load_si256((__m256i*)(ptr_src0+iw+ si_w));
            y2 = _mm256_load_si256((__m256i*)(ptr_src1+iw+ 0));
            y3 = _mm256_load_si256((__m256i*)(ptr_src1+iw+ si_w));
            y4 = y0;
            y4 = _mm256_or_si256(y4, y1);
            y4 = _mm256_or_si256(y4, xPbMaskf3);
            y4 = _mm256_and_si256(y4, y0);
            y5 = y2;
            y5 = _mm256_or_si256(y5, y3);
            y5 = _mm256_or_si256(y5, xPbMaskf3);
            y5 = _mm256_and_si256(y5, y2);
            y4 = _mm256_and_si256(y4, y5);
            y4 = _mm256_and_si256(y4, xPbMask44);
            y5 = y0;
            y5 = _mm256_andnot_si256(y5, xPbMask33);
            y4 = _mm256_or_si256(y4, y5);
            _mm256_store_si256((__m256i*)(ptr_dst+iw), y4);

            _mm256_store_si256((__m256i*)(bufptr_0           +iw), y0);
            _mm256_store_si256((__m256i*)(bufptr_1           +iw), y1);
            _mm256_store_si256((__m256i*)(bufptr_0+block_size+iw), y2);
            _mm256_store_si256((__m256i*)(bufptr_1+block_size+iw), y3);
        }
        ptr_dst += si_w;
        ptr_src0 += si_w;
        ptr_src1 += si_w;

        for (int ih = h - 2; ih; ih--) {
            for (int iw = 0; iw < step; iw += 32) {
                y4 = _mm256_load_si256((__m256i*)(bufptr_0+           iw));
                y0 = _mm256_load_si256((__m256i*)(bufptr_1+           iw));
                y5 = _mm256_load_si256((__m256i*)(bufptr_0+block_size+iw));
                y2 = _mm256_load_si256((__m256i*)(bufptr_1+block_size+iw));
                y1 = _mm256_load_si256((__m256i*)(ptr_src0 + si_w + iw));
                y3 = _mm256_load_si256((__m256i*)(ptr_src1 + si_w + iw));
                _mm_prefetch((char *)(ptr_src0 + si_w * 2), _MM_HINT_NTA);
                _mm_prefetch((char *)(ptr_src1 + si_w * 2), _MM_HINT_NTA);
                _mm256_store_si256((__m256i*)(bufptr_0           +iw), y1);
                _mm256_store_si256((__m256i*)(bufptr_0+block_size+iw), y3);
                y4 = _mm256_or_si256(y4, y1);
                y4 = _mm256_or_si256(y4, xPbMaskf3);
                y4 = _mm256_and_si256(y4, y0);
                y5 = _mm256_or_si256(y5, y3);
                y5 = _mm256_or_si256(y5, xPbMaskf3);
                y5 = _mm256_and_si256(y5, y2);
                y4 = _mm256_and_si256(y4, y5);
                y4 = _mm256_and_si256(y4, xPbMask44);
                y5 = y0;
                y5 = _mm256_andnot_si256(y5, xPbMask33);
                y4 = _mm256_or_si256(y4, y5);
                _mm256_store_si256((__m256i*)(ptr_dst+iw), y4);
            }
            SWAP(BYTE *, bufptr_0, bufptr_1);
            ptr_dst += si_w;
            ptr_src0 += si_w;
            ptr_src1 += si_w;
        }
        for (int iw = 0; iw < step; iw += 32) {
            y4 = _mm256_load_si256((__m256i*)(bufptr_0+           iw));
            y1 = _mm256_load_si256((__m256i*)(bufptr_1+           iw));
            y5 = _mm256_load_si256((__m256i*)(bufptr_0+block_size+iw));
            y3 = _mm256_load_si256((__m256i*)(bufptr_1+block_size+iw));
            y4 = _mm256_or_si256(y4, y1);
            y4 = _mm256_or_si256(y4, xPbMaskf3);
            y4 = _mm256_and_si256(y4, y1);
            y5 = _mm256_or_si256(y5, y3);
            y5 = _mm256_or_si256(y5, xPbMaskf3);
            y5 = _mm256_and_si256(y5, y3);
            y4 = _mm256_and_si256(y4, y5);
            y4 = _mm256_and_si256(y4, xPbMask44);
            y5 = y1;
        
            y5 = _mm256_andnot_si256(y5, xPbMask33);
            y4 = _mm256_or_si256(y4, y5);
            _mm256_store_si256((__m256i*)(ptr_dst+iw), y4);
        }
    }
    _mm256_zeroupper();
 }    

void __stdcall afs_analyzemap_filter_avx2_plus(BYTE* sip, int si_w, int w, int h) {
    __m256i y0, y1, y2, y3, y4, y5;
    __m256i y6 = _mm256_load_si256((__m256i*)(pb_mask_03));
    __m256i y7 = _mm256_load_si256((__m256i*)(pb_mask_04));
    BYTE *ptr_sip, *ptr_sip_line;
    BYTE *buf_ptr;
    int iw, jh;
    const int BLOCK_SIZE = 4096;
    BYTE __declspec(align(32)) buffer[BLOCK_SIZE];
    //この関数ではsi_wがBLOCK_SIZEまでしか処理できないので、それ以上なら旧関数を使う
    if (si_w > BLOCK_SIZE)
        return afs_analyzemap_filter_avx2(sip, si_w, w, h);

    ////// loop_1 ////////////////////////////////////////////////////////////////////
    ptr_sip_line = sip;
    ptr_sip = ptr_sip_line;
    buf_ptr = buffer;
    //---- loop_1 - prepare for first line ---------------------------------------------
    y2 = _mm256_load_si256((__m256i*)(ptr_sip+ 0));
    y1 = _mm256_alignr256_epi8(y2, _mm256_setzero_si256(), 1);
    for (iw = w; iw > 32; iw -= 32, ptr_sip += 32, buf_ptr += 32) {
        //horizontal filtering
        y0 = y1;
        y1 = y2;
        y2 = _mm256_load_si256((__m256i*)(ptr_sip+32));
        _mm_prefetch((char *)ptr_sip+si_w, _MM_HINT_T0);
            
        y3 = _mm256_alignr256_epi8(y1, y0, 31);
        y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
        y5 = y3;
        y3 = _mm256_or_si256(y3, y4);
        y5 = _mm256_and_si256(y5, y4);
        y3 = _mm256_and_si256(y3, y6);
        y5 = _mm256_and_si256(y5, y7);
        y3 = _mm256_or_si256(y3, y1);
        y3 = _mm256_or_si256(y3, y5);
        _mm256_store_si256((__m256i*)(ptr_sip), y3);
        _mm256_store_si256((__m256i*)buf_ptr, y3);
    }
    //---- loop_1 - first line - last edge
    y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
    y4 = y2;
    y4 = _mm256_and_si256(y4, y0);
    y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
    y3 = _mm256_alignr256_epi8(y2, y1, 31);
    y5 = y2;
    y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
    y4 = _mm256_or_si256(y4, y5);
        
    y5 = y3;
    y3 = _mm256_or_si256(y3, y4);
    y5 = _mm256_and_si256(y5, y4);
    y3 = _mm256_and_si256(y3, y6);
    y5 = _mm256_and_si256(y5, y7);
    y3 = _mm256_or_si256(y3, y2);
    y3 = _mm256_or_si256(y3, y5);
    y3 = _mm256_and_si256(y3, y0);
        
    _mm256_store_si256((__m256i*)(ptr_sip), y3);
    _mm256_store_si256((__m256i*)buf_ptr, y3);
    //---- loop_1 - prepare for first line - end ---------------------------------------------

    //---- loop_1 - main loop  ---------------------------------------------
    for (jh = 1; jh < h; jh++, ptr_sip_line += si_w) {
        buf_ptr = buffer;
        ptr_sip = ptr_sip_line;
        y2 = _mm256_load_si256((__m256i*)(ptr_sip+si_w+ 0));
        y1 = _mm256_alignr256_epi8(y2, _mm256_setzero_si256(), 1);
        for (iw = w; iw > 32; iw -= 32, ptr_sip += 32, buf_ptr += 32) {
            //horizontal filtering
            y0 = y1;
            y1 = y2;
            y2 = _mm256_load_si256((__m256i*)(ptr_sip+si_w+32));
            _mm_prefetch((char *)ptr_sip+si_w*2, _MM_HINT_T0);
            
            y3 = _mm256_alignr256_epi8(y1, y0, 31);
            y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
            y5 = y3;
            y3 = _mm256_or_si256(y3, y4);
            y5 = _mm256_and_si256(y5, y4);
            y3 = _mm256_and_si256(y3, y6);
            y5 = _mm256_and_si256(y5, y7);
            y3 = _mm256_or_si256(y3, y1);
            y3 = _mm256_or_si256(y3, y5);
            _mm256_store_si256((__m256i*)(ptr_sip+si_w), y3);

            //vertical filtering
            y4 = _mm256_load_si256((__m256i *)buf_ptr);
            y5 = _mm256_load_si256((__m256i *)ptr_sip);

            y3 = _mm256_and_si256(y3, y4);
            y3 = _mm256_and_si256(y3, _mm256_or_si256(y6, y7));
            y3 = _mm256_or_si256(y3, y5);
            _mm256_store_si256((__m256i*)ptr_sip, y3);
            _mm256_store_si256((__m256i*)buf_ptr, y5);
        }
        //---- loop_1 - main loop - last edge ---------------------------
        //horizontal filtering
        y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
        y4 = y2;
        y4 = _mm256_and_si256(y4, y0);
        y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
        y3 = _mm256_alignr256_epi8(y2, y1, 31);
        y5 = y2;
        y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
        y4 = _mm256_or_si256(y4, y5);
        
        y5 = y3;
        y3 = _mm256_or_si256(y3, y4);
        y5 = _mm256_and_si256(y5, y4);
        y3 = _mm256_and_si256(y3, y6);
        y5 = _mm256_and_si256(y5, y7);
        y3 = _mm256_or_si256(y3, y2);
        y3 = _mm256_or_si256(y3, y5);
        y3 = _mm256_and_si256(y3, y0);
        
        _mm256_store_si256((__m256i*)(ptr_sip+si_w), y3);
        
        //vertical filtering
        y4 = _mm256_load_si256((__m256i *)buf_ptr);
        y5 = _mm256_load_si256((__m256i *)ptr_sip);

        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_and_si256(y3, _mm256_or_si256(y6, y7));
        y3 = _mm256_or_si256(y3, y5);
        _mm256_store_si256((__m256i*)ptr_sip, y3);
        _mm256_store_si256((__m256i*)buf_ptr, y5);
        //---- loop_1 - main loop - last edge end ------------------------
    }
    //---- loop_1 - main loop  - end  ------------------------------------------

    //---- loop_1 - last line --------------------------------------------
    buf_ptr = buffer;
    ptr_sip = ptr_sip_line;
    for (iw = 0; iw < si_w; iw += 32, ptr_sip += 32, buf_ptr += 32) {
        //vertical filtering
        y4 = _mm256_load_si256((__m256i *)buf_ptr);
        y5 = _mm256_load_si256((__m256i *)ptr_sip);
        y3 = y5;

        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_and_si256(y3, _mm256_or_si256(y6, y7));
        y3 = _mm256_or_si256(y3, y5);
        _mm256_store_si256((__m256i*)ptr_sip, y3);
    }
    //---- loop_1 - last line - end ---------------------------------------------
    ////// loop_1 - end  ////////////////////////////////////////////////////////////////////

    ////// loop_2 ////////////////////////////////////////////////////////////////////
    ptr_sip_line = sip;
    ptr_sip = ptr_sip_line;
    buf_ptr = buffer;
    y6 = _mm256_xor_si256(_mm256_cmpeq_epi8(y6, y6), _mm256_or_si256(y6, y7));
    //---- loop_2 - prepare for first line --------------------------------------------
    y2 = _mm256_load_si256((__m256i*)(ptr_sip+ 0));
    y1 = _mm256_alignr256_epi8(y2, _mm256_setzero_si256(), 1);
    for (iw = w; iw > 32; iw -= 32, ptr_sip += 32, buf_ptr += 32) {
        //horizontal filtering
        y0 = y1;
        y1 = y2;
        y2 = _mm256_load_si256((__m256i*)(ptr_sip+32));
        _mm_prefetch((char *)ptr_sip+si_w, _MM_HINT_T0);
            
        y3 = _mm256_alignr256_epi8(y1, y0, 31);
        y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y1);
        _mm256_store_si256((__m256i*)(ptr_sip), y3);
        _mm256_store_si256((__m256i*)buf_ptr, y3);
    }
    //first line - last edge
    y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
        
    y4 = y2;
    y4 = _mm256_and_si256(y4, y0);
    y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
    y3 = _mm256_alignr256_epi8(y2, y1, 31);
    y5 = y2;
    y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
    y4 = _mm256_or_si256(y4, y5);
        
    y3 = _mm256_and_si256(y3, y4);
    y3 = _mm256_or_si256(y3, y6);
    y3 = _mm256_and_si256(y3, y2);
    y3 = _mm256_and_si256(y3, y0);
        
    _mm256_store_si256((__m256i*)(ptr_sip), y3);
    _mm256_store_si256((__m256i*)buf_ptr, y3);
    //---- loop_2 - prepare for first line - end ---------------------------------------------

    //---- loop_2 - main loop  ---------------------------------------------
    for (jh = 1; jh < h; jh++, ptr_sip_line += si_w) {
        buf_ptr = buffer;
        ptr_sip = ptr_sip_line;
        y2 = _mm256_load_si256((__m256i*)(ptr_sip+si_w+ 0));
        y1 = _mm256_alignr256_epi8(y2, _mm256_setzero_si256(), 1);
        for (iw = w; iw > 32; iw -= 32, ptr_sip += 32, buf_ptr += 32) {
            //horizontal filtering
            y0 = y1;
            y1 = y2;
            y2 = _mm256_load_si256((__m256i*)(ptr_sip+si_w+32));
            _mm_prefetch((char *)ptr_sip+si_w*2, _MM_HINT_T0);
            
            y3 = _mm256_alignr256_epi8(y1, y0, 31);
            y4 = _mm256_alignr256_epi8(y2, y1, 1);
            
            y3 = _mm256_and_si256(y3, y4);
            y3 = _mm256_or_si256(y3, y6);
            y3 = _mm256_and_si256(y3, y1);
            _mm256_store_si256((__m256i*)(ptr_sip+si_w), y3);

            //vertical filtering
            y4 = _mm256_load_si256((__m256i *)buf_ptr);
            y5 = _mm256_load_si256((__m256i *)ptr_sip);

            y3 = _mm256_and_si256(y3, y4);
            y3 = _mm256_or_si256(y3, y6);
            y3 = _mm256_and_si256(y3, y5);
            _mm256_store_si256((__m256i*)ptr_sip, y3);
            _mm256_store_si256((__m256i*)buf_ptr, y5);
        }
        //---- loop_2 - main loop - last edge --------------------------
        //horizontal filtering
        y0 = _mm256_load_si256((__m256i*)(pqb_mask_a + iw*32));
        
        y4 = y2;
        y4 = _mm256_and_si256(y4, y0);
        y4 = _mm256_alignr256_epi8(_mm256_setzero_si256(), y4, 1);
        y3 = _mm256_alignr256_epi8(y2, y1, 31);
        y5 = y2;
        y5 = _mm256_and_si256(y5, _mm256_load_si256((__m256i*)(pqb_mask_s + iw*32)));
        y4 = _mm256_or_si256(y4, y5);
        
        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y2);
        y3 = _mm256_and_si256(y3, y0);
        
        _mm256_store_si256((__m256i*)(ptr_sip+si_w), y3);
        
        //vertical filtering
        y4 = _mm256_load_si256((__m256i *)buf_ptr);
        y5 = _mm256_load_si256((__m256i *)ptr_sip);

        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y5);
        _mm256_store_si256((__m256i*)ptr_sip, y3);
        _mm256_store_si256((__m256i*)buf_ptr, y5);
        //---- loop_2 - main loop - last edge end ------------------------
    }
    //---- loop_2 - main loop  - end  ------------------------------------------

    //---- loop_2 - last line ---------------------------------------------
    buf_ptr = buffer;
    ptr_sip = ptr_sip_line;
    for (iw = 0; iw < si_w; iw += 32, ptr_sip += 32, buf_ptr += 32) {
        //vertical filtering
        y4 = _mm256_load_si256((__m256i *)buf_ptr);
        y5 = _mm256_load_si256((__m256i *)ptr_sip);
        y3 = y5;
        
        y3 = _mm256_and_si256(y3, y4);
        y3 = _mm256_or_si256(y3, y6);
        y3 = _mm256_and_si256(y3, y5);
        _mm256_store_si256((__m256i*)ptr_sip, y3);
    }
    //---- loop_2 - last line - end ---------------------------------------------
    ////// loop_2 - end  ////////////////////////////////////////////////////////////////////
    _mm256_zeroupper();
}

